{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll walk through downloading and preprocessing the compendium of ENCODE and Epigenomics Roadmap data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This part won't be very iPython tutorial-ly...\n",
    "\n",
    "First cd in the terminal over to the data directory and run the script *get_dnase.sh*.\n",
    "\n",
    "That will download all of the BED files from ENCODE and Epigenomics Roadmap. Read the script to see where I'm getting those files from. Perhaps there will be more in the future, and you'll want to manipulate the links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that has finished, we need to merge all of the BED files into one BED and an activity table.\n",
    "\n",
    "I typically use the -y option to avoid the Y chromosome, since I don't know which samples sequenced male or female cells.\n",
    "\n",
    "I'll use my default of extending the sequences to 600 bp, and merging sites that overlap by more than 200 bp. But you might want to edit these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd ../data; preprocess_features.py -y -m 200 -s 600 -o er -c genomes/human.hg19.genome sample_beds.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the sequences to the format needed by Torch, we'll first convert to FASTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!bedtools getfasta -fi ../data/genomes/hg19.fa -bed ../data/er.bed -s -fo ../data/er.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert to HDF5 for Torch and set aside some data for validation and testing.\n",
    "\n",
    "-r permutes the sequences.\n",
    "-c informs the script we're providing raw counts.\n",
    "-v specifies the size of the validation set.\n",
    "-t specifies the size of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring header line\n",
      "1880000 training sequences \n",
      "71886 test sequences \n",
      "70000 validation sequences \n"
     ]
    }
   ],
   "source": [
    "!seq_hdf5.py -c -r -t 71886 -v 70000 ../data/er.fa ../data/er_act.txt ../data/er.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you're good to go!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
